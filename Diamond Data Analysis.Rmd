---
title: "Diamond Data Analysis"
author: "Ally Weingarden, Juliet Niebylski, Ziying Peng, Leah Sun, Marisa Wong"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load Libraries

```{r}
library(ggplot2)
library(caret)
library(neuralnet)
library(rpart)
library(kernlab)
library(randomForest)
```

### Load in Data

```{r, cache = TRUE}
diamonds <- diamonds
summary(diamonds)
str(diamonds)
nrow(diamonds)
```

Data is from the ggplot2 library in R.

For the second group project, we have chosen a dataset on diamonds. This dataset has 10 variables which includes the price as well as various characteristics about diamonds. There are 53940 observations of data in this dataset. Our business question is: how should we price diamonds based on certain characteristics? We will answer this question by creating models to predict the price using a variety of characteristics about diamonds. This addresses the business problem of setting prices for a seller's inventory of diamonds since it is important to set reasonable prices based on what other similar diamonds are selling for.

### Clean Data

```{r, cache = TRUE}
# Make certain variables to be a factor 
# cut, color, clarity 
diamonds$cut <- factor(diamonds$cut, ordered = FALSE)
diamonds$color <- factor(diamonds$color, ordered = FALSE)
diamonds$clarity <- factor(diamonds$clarity, ordered = FALSE)

str(diamonds)
summary(diamonds)
```

Before moving to the analysis of the data set, we cleaned the data to prepare it for regression. Cut, color, and clarity were ordinal factors. For creating our models, we decided to change cut, color, and clarity to be factors which are not ordered.

### Regression

```{r, cache + TRUE}
diamonds_model <- lm(price ~ ., data = diamonds)
summary(diamonds_model)
```

We ran a linear regression model on the cleaned data with the response variable price and included all predictor variables. From doing this, we see that only variables y and z are not statistically significant at an alpha level of 0.05. All other predictor variables are statistically significant at an alpha level of 0.05 (and all of those are even statistically significant at an alpha level of 0.001). Because of having many statistically significant predictor variables at an alpha level of 0.05, we decided to go ahead and choose this dataset to use in this project to predict the price of diamonds.

### Split Data into Training and Testing Data

```{r, cache = TRUE}
# Get value of 70% of the data
num_training_data <- round(0.7 * nrow(diamonds))

# Get random sample of indexes for the training data
set.seed(123) # Set a seed
train_index <- sample(x = c(1:nrow(diamonds)), size = num_training_data, replace = FALSE)

# Get training data based on the train_index values randomly selected
train_diamonds <- diamonds[train_index,]
# Get testing data
test_diamonds <- diamonds[-train_index,]
# Check: make sure the number of values in training set and testing set 
# add to the number of rows in the original dataset
nrow(diamonds) == nrow(train_diamonds) + nrow(test_diamonds)
# Get summary of training and test data, make sure similar distribution of price values
summary(train_diamonds)
summary(test_diamonds)
```

### Linear Regression 

```{r, cache = TRUE}
plot( ~ price + carat + depth + table + x + y + z, data = train_diamonds)
#quadratic relationship between x and price

hist(train_diamonds$price)
hist(train_diamonds$carat)
hist(train_diamonds$depth)
hist(train_diamonds$table)
hist(train_diamonds$x)
hist(train_diamonds$y)
hist(train_diamonds$z)
# carat has a heavy right skew


# Main effects model
model1 <- lm(price ~ ., train_diamonds)
summary(model1)
prediction_values_lm1 <- predict(model1, test_diamonds)
summary(prediction_values_lm1)
summary(actual_vs_expected_diff_lm1)
postResample(prediction_values_lm1, test_diamonds$price)
#       RMSE     Rsquared          MAE 
# 1162.9890409    0.9158249  744.0980344

#Step Model
model2 <- step(model1, direction = "backward", trace = 0)
summary(model2)
prediction_values_lm2 <- predict(model2, test_diamonds)
summary(prediction_values_lm2)
summary(actual_vs_expected_diff_lm2)
postResample(prediction_values_lm2, test_diamonds$price)
#just removes y
# RMSE     Rsquared          MAE 
# 1162.7095378    0.9158652  744.0409995 

# Interactions
model3 <- lm(price ~ clarity + color + cut + log(carat) + depth + table + x + z + clarity*cut + clarity*color + color*cut + carat*color + depth*cut + depth*color + x*depth + z*depth + I(x^2), train_diamonds)
summary(model3)
prediction_values_lm3 <- predict(model3, test_diamonds)
summary(prediction_values_lm3)
postResample(prediction_values_lm3, test_diamonds$price)
#  RMSE     Rsquared          MAE 
# 1033.2668851    0.9335579  628.8434045 

```

### KNN Regression

```{r, cache = TRUE}
knnreg_model1 <- knnreg(price ~ ., data = train_diamonds, k = 5)
summary(knnreg_model1)
prediction_values_knn1 <- predict(knnreg_model1, test_diamonds)
summary(prediction_values_knn1)
# actual_vs_expected_diff_knn1 <- test_diamonds$price - prediction_values_knn1
# summary(actual_vs_expected_diff_knn1)
postResample(prediction_values_knn1, test_diamonds$price)
#        RMSE     Rsquared          MAE 
#1037.3289972    0.9343628  540.2361728 

# # Improving model through changing k
# knnreg_model2 <- knnreg(price ~ ., data = train_diamonds, k = 10)
# prediction_values_knn2 <- predict(knnreg_model2, test_diamonds)
# actual_vs_expected_diff_knn2 <- test_diamonds$price - prediction_values_knn2
# 
# postResample(prediction_values_knn2, test_diamonds$price)
# #        RMSE     Rsquared          MAE 
# #1056.7784212    0.9343528  553.3987039 
# 
# knnreg_model3 <- knnreg(price ~ ., data = train_diamonds, k = 7)
# prediction_values_knn3 <- predict(knnreg_model3, test_diamonds)
# actual_vs_expected_diff_knn3 <- test_diamonds$price - prediction_values_knn3
# 
# postResample(prediction_values_knn3, test_diamonds$price)
# #        RMSE     Rsquared          MAE 
# #1039.6573549    0.9349978  542.4383039 
# 
# knnreg_model4 <- knnreg(price ~ ., data = train_diamonds, k = 20)
# prediction_values_knn4 <- predict(knnreg_model4, test_diamonds)
# actual_vs_expected_diff_knn4 <- test_diamonds$price - prediction_values_knn4
# 
# postResample(prediction_values_knn4, test_diamonds$price)
# #        RMSE     Rsquared          MAE 
# #1100.2318281    0.9311135  580.5150942 

# Train the model to find kval
library(caret)
set.seed(300)
k <- train(price ~ ., data = train_diamonds, method = "knn")
# summary of tuning results
k
# k = 9 best
# k  RMSE      Rsquared   MAE     
#   5  1152.713  0.9172425  612.3353
#   7  1133.521  0.9209492  604.8246
#   9  1125.447  0.9229289  601.9236
knnreg_model_train <- knnreg(price ~ ., data = train_diamonds, k = 9)
summary(knnreg_model_train)
prediction_values_train_knn <- predict(knnreg_model_train, test_diamonds)
summary(prediction_values_train_knn)
postResample(prediction_values_train_knn, test_diamonds$price)
#        RMSE     Rsquared          MAE 
#1049.7342230    0.9348145  550.5465499 
```

# ANN 

```{r, cache = TRUE}
# need to break into dummy variables
library(gmodels)
library(caret)
library(neuralnet)

train_ANN <- as.data.frame(model.matrix(~.-1, train_diamonds))
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
train_ANN_nor <- as.data.frame(lapply(train_ANN, normalize))
test_ANN_nor <- as.data.frame(lapply(test_ANN, normalize))


test_ANN <- as.data.frame(model.matrix(~.-1,test_diamonds))
str(train_ANN)
str(test_ANN)

# Model 1 (Baseline)
ANN_model1 <- neuralnet(formula = price ~ ., data = train_ANN_nor)
plot(ANN_model1)
prediction_ANN_1 <- predict(ANN_model1, newdata = test_ANN_nor)
binaryprediction_1 <- ifelse(prediction_ANN_1 > 0.5, 1, 0)
# Create Confusion Matrix
CrossTable(x = test_ANN_nor$price, y = binaryprediction_1, propsq = FALSE)
# Evaluate confusion matrix
(confusion_matrix_model_ANN1 <- confusionMatrix(as.factor(binaryprediction_1), as.factor(test_ANN_nor$price), positive = "1"))
# Sensitivity:    ; Pos Pred Value:


# # Model 2 (Adding Nodes)
# ANN_model2 <- neuralnet(formula = price ~ ., data = train_diamonds, hidden = 3)
# plot(ANN_model2)
# prediction_ANN_2 <- predict(ANN_model2, newdata = test_diamonds)
# binaryprediction_2 <- ifelse(prediction_ANN_2 > 0.5, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_diamonds$price, y = binaryprediction_2, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model_ANN2 <- confusionMatrix(as.factor(binaryprediction_2), as.factor(test_diamonds$price), positive = "1"))
 


# # Model 3 (Adding Layers)
# ANN_model3 <- neuralnet(formula = price ~ ., data = train_diamonds, hidden = c(2,3), threshold = 0.01, stepmax = 10^5)
# plot(ANN_model3)
# prediction_ANN_3 <- predict(ANN_model3, newdata = test_diamonds)
# binaryprediction_3 <- ifelse(prediction_ANN_3 > 0.5, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_diamonds$price, y = binaryprediction_3, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model_ANN3 <-confusionMatrix(as.factor(binaryprediction_3), as.factor(test_diamonds$price), positive = "1"))


# # Model 4 (Adjusting Binary Prediction Definition)
# ANN_model4 <- neuralnet(formula = price ~ ., data = train_diamonds)
# plot(ANN_model4)
# prediction_ANN_4 <- predict(ANN_model4, newdata = test_diamonds)
# binaryprediction_4 <- ifelse(prediction_ANN_4 > 0.4, 1, 0)
# # Create Confusion Matrix
# CrossTable(x = test_diamonds$price, y = binaryprediction_4, propsq = FALSE)
# # Evaluate confusion matrix
# (confusion_matrix_model_ANN4 <-confusionMatrix(as.factor(binaryprediction_4), as.factor(test_diamonds$price), positive = "1"))
```

### SVM

```{r, cache = TRUE}
#SVM_model1 <- ksvm(price ~ ., data = train_diamonds, kernel = "vanilladot")
ctrl_svm <- trainControl(method = "cv", 10)
#tune_grid_svm <- data.frame("mtry" = c(1:9))
SVM_model1 <- train(price ~ ., data = train_diamonds, method = "svmLinear",
                 trControl = ctrl_svm
                 )
prediction_values_SVM1 <- predict(SVM_model1, test_diamonds)
postResample(prediction_values_SVM1, test_diamonds$price)
```

### Regression Tree

- look into improving regression tree

Info about regression trees: https://www.datadriveninvestor.com/2020/04/13/how-do-regression-trees-work/

```{r, cache = TRUE}
# One regression tree made with training data to predict price of testing data
reg_tree_1 <- rpart(formula = price ~ ., data = train_diamonds)
prediction_values_reg_tree1 <- predict(reg_tree_1, test_diamonds)
summary(prediction_values_reg_tree1) # only positive prices predicted
postResample(prediction_values_reg_tree1, test_diamonds$price)
# RMSE = 1423.7770949
# Rsquared = 0.8738465
# MAE = 909.6853036
```

### Random Forest

#### One random forest made with training data to predict price of testing data using 10 trees

```{r, cache = TRUE}
random_forest_model1 <- randomForest(price ~ ., data = train_diamonds, ntree = 10)
prediction_values_random_forest1 <- predict(random_forest_model1, test_diamonds)
summary(prediction_values_random_forest1) # only positive prices predicted
postResample(prediction_values_random_forest1, test_diamonds$price)
# RMSE = 625.1970581
# Rsquared = 0.9758139
# MAE = 307.6215889
```

#### One random forest made with training data to predict price of testing data using 100 trees

```{r, cache = TRUE}
random_forest_model2 <- randomForest(price ~ ., data = train_diamonds, ntree = 100)
prediction_values_random_forest2 <- predict(random_forest_model2, test_diamonds)
summary(prediction_values_random_forest2) # only positive prices predicted
postResample(prediction_values_random_forest2, test_diamonds$price)
# RMSE = 588.6442348
# Rsquared = 0.9785971
# MAE = 288.6016663
```

#### One random forest made with training data to predict price of testing data using 500 trees

```{r, cache = TRUE}
random_forest_model3 <- randomForest(price ~ ., data = train_diamonds)
prediction_values_random_forest3 <- predict(random_forest_model1, test_diamonds)
summary(prediction_values_random_forest3) # only positive prices predicted
postResample(prediction_values_random_forest3, test_diamonds$price)
# RMSE = 625.1970581
# Rsquared = 0.9758139
# MAE = 307.6215889
```

#### Tuning random forest

method info for train function: https://topepo.github.io/caret/train-models-by-tag.html 
Random forest info for train function: https://topepo.github.io/caret/train-models-by-tag.html#random-forest 

- could try train function with different split of training and testing data (70/30%) and use ntree = 100
- look into if can test different ntree amounts using train function

```{r, cache = TRUE}
#ntree_rf <- data.frame("ntree" = seq(10, 1000, 10))
tune_grid_rf <- data.frame("mtry" = c(1:9))
ctrl_rf <- trainControl(method = "cv", 10) # use 10-fold cross-validation
set.seed(12345)
#random_forests <- train(price ~ ., data = diamonds, method = "rf", tuneGrid = tune_grid_rf, trControl = ctrl_rf, ntree = ntree_rf)
random_forests <- train(price ~ ., data = diamonds, method = "rf", tuneGrid = tune_grid_rf, trControl = ctrl_rf, ntree = 10)
random_forests

# Using RMSE, mtry = 9 was chosen (RMSE = 646.2072, Rsquared = 0.9737722, MAE = 311.5486)
```

### Stacked Model
```{r}
# stack_model <- data.frame(prediction models, test_diamonds$price)
# summary(stack_model)
# str(stack_model)
```

## New Test and Train
```{r}
stack_test_set <- sample(1:nrow(stack_model), nrow(stack_model) * 0.3) 
stack_train <- stack_model[-stack_test_set, ]
stack_test <- stack_model[stack_test_set, ]
```

## Stack Decision Tree
```{r}
library(C50)
library(gmodels)
library(caret)
stack_dt <- C5.0(as.factor(test_diamonds.price) ~ ., data = stack_train) 
plot(stack_dt)
stack_dt
summary(stack_dt)
stack_predictions_dt <- predict(stack_dt, stack_test)
postResample(stack_predictions_dt, stack_test$price)

```

## Cost Matrix
```{r}
error_cost <- matrix(c(0,1,4,0), nrow=2)
error_cost
stack_dt_cost <- C5.0(as.factor(tele_diamonds.price) ~ .,   data = stack_train, costs = error_cost)
plot(stack_dt_cost)
stack_cost_predictions <- predict(stack_dt_cost,stack_test)
postResample(stack_cost_predictions, stack_test$price)

```

QUESTIONS:
1. Why are there negative prices predicted? 

ways to censor the data, don't worry about this right now (for questions 1 and 2)

2. Is there a way to ensure that the price is positive?

3. How to test goodness of estimation?

caret package: use RMSE (lower is better), MAE (lower is better), R^2 (larger is better) postResample in Caret (will calculate all three metrics)

Resources:
RMSE: https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/
MAE: https://www.statisticshowto.com/absolute-error/ 
R^2: https://www.investopedia.com/terms/r/r-squared.asp 

4. Not sure how accurate the data is since couldn't find info about using the letter color scale and clarity coding that the dataset uses for cubic zirconia (seems like they are using diamond coding). Should we instead use the diamond dataset the professor had mentioned?

switch over to diamonds data from ggplot2, don't need to redo the first deliverable

5. When are the other deliverables due?

- second deliverable due sometime before Thanksgiving

Model specifications:

- Linear models (can include interactions and non-linear terms) <- Ally
- KNN reg package, use KNNreg function in caret <- Ziying
- neural network (use as normal) <- Leah
- decision tree (regression trees), package CART or PARTYCIT RPART  <- Juliet
- SVM (support vector machines) (try to use as normal) <- Marisa
- random forest for continuous data, RPART or CART <- Juliet

Regression tree resource: https://www.geeksforgeeks.org/decision-tree-for-regression-in-r-programming/

Aside: Resources on Cubic Zirconia:
https://www.firemountaingems.com/resources/encyclobeadia/gem-notes/hb1f

https://www.thespruce.com/cubic-zirconia-information-2043824 

https://www.jewelry-secrets.com/Blog/cubic-zirconia-grading/ 

Info about diamonds: https://www.forevermark.com/en-us/now-forever/guides/diamond-engagement-guide/diamond-color-and-clarity/
