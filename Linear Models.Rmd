---
title: "Linear Models"
author: "Ally Weingarden"
date: "2022-11-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(ggplot2)
library(caret)
```

```{r}
diamonds <- diamonds
summary(diamonds)
str(diamonds)
```

# Clean Data

```{r}
diamonds$cut <- factor(diamonds$cut, ordered = FALSE)
diamonds$color <- factor(diamonds$color, ordered = FALSE)
diamonds$clarity <- factor(diamonds$clarity, ordered = FALSE)

str(diamonds)
summary(diamonds)
```


# Regression

```{r}
diamonds_model <- lm(price ~ ., data = diamonds)
summary(diamonds_model)
```


# Split Data into Training and Testing Data

```{r}
# Get value of 70% of the data
num_training_data <- round(0.7 * nrow(diamonds))

# Get random sample of indexes for the training data
set.seed(123) # Set a seed
train_index <- sample(x = c(1:nrow(diamonds)), size = num_training_data, replace = FALSE)

# Get training data based on the train_index values randomly selected
train_diamonds <- diamonds[train_index,]
# Get testing data
test_diamonds <- diamonds[-train_index,]
# Check: make sure the number of values in training set and testing set 
# add to the number of rows in the original dataset
nrow(diamonds) == nrow(train_diamonds) + nrow(test_diamonds)
# Get summary of training and test data, make sure similar distribution of price values
summary(train_diamonds)
summary(test_diamonds)
```


```{r}
plot( ~ price + carat + depth + table + x + y + z, data = train_diamonds)
#quadtraic relationship between x and price

hist(train_diamonds$price)
hist(train_diamonds$carat)
hist(train_diamonds$depth)
hist(train_diamonds$table)
hist(train_diamonds$x)
hist(train_diamonds$y)
hist(train_diamonds$z)
# carat has a heavy right skew

library(ggplot2)
library(jtools)
library(interactions)
fit1 <- lm(price ~ carat * cut, data = train_diamonds)
interact_plot(fit1, pred = carat, modx = cut)
#interaction not necessary
fit2 <- lm(price ~ cut * depth, data = train_diamonds)
interact_plot(fit2, pred = cut, modx = depth)
fit3 <- lm(price ~ x * carat, data = train_diamonds)
interact_plot(fit3, pred = x, modx = carat)
# interaction not necessary
fit4 <- lm(price ~ z * carat, data = train_diamonds)
interact_plot(fit4, pred = z, modx = carat)
# interaction not necessary
fit5 <- lm(price ~ x * depth, data = train_diamonds)
interact_plot(fit5, pred = x, modx = depth)
# interaction not necessary
fit6 <- lm(price ~ z * depth, data = train_diamonds)
interact_plot(fit6, pred = z, modx = depth)
# interaction not necessary
fit7 <- lm(price ~ x * cut, data = train_diamonds)
interact_plot(fit7, pred = x, modx = cut)
# interaction not necessary
fit8 <- lm(price ~ z * cut, data = train_diamonds)
interact_plot(fit8, pred = z, modx = cut)
# interaction not necessary
fit9 <- lm(price ~ depth * carat, data = train_diamonds)
interact_plot(fit9, pred = depth, modx = carat)
# interaction necessary


# Main effects model
model1 <- lm(price ~ ., train_diamonds)
summary(model1)
prediction_values_lm1 <- predict(model1, test_diamonds)
summary(prediction_values_lm1)
actual_vs_expected_diff_lm1 <- test_diamonds$price - prediction_values_lm1
summary(actual_vs_expected_diff_lm1)
postResample(prediction_values_lm1, test_diamonds$price)
#       RMSE     Rsquared          MAE 
# 1162.9890409    0.9158249  744.0980344
# 5.862 mean difference

#Step Model
model2 <- step(model1, direction = "backward", trace = 0)
summary(model2)
prediction_values_lm2 <- predict(model2, test_diamonds)
summary(prediction_values_lm2)
actual_vs_expected_diff_lm2 <- test_diamonds$price - prediction_values_lm2
summary(actual_vs_expected_diff_lm2)
postResample(prediction_values_lm2, test_diamonds$price)
#just removes y
# RMSE     Rsquared          MAE 
# 1162.7095378    0.9158652  744.0409995 
# 5.781 mean difference

# Interactions
model3 <- lm(price ~ clarity + color + cut + log(carat) + depth + table + x + z + clarity*cut + clarity*color + color*cut + carat*color + depth*cut + depth*color + x*depth + z*depth + I(x^2), train_diamonds)
summary(model3)
prediction_values_lm3 <- predict(model3, test_diamonds)
summary(prediction_values_lm3)
postResample(prediction_values_lm3, test_diamonds$price)
#  RMSE     Rsquared          MAE 
# 1033.2668851    0.9335579  628.8434045 
# 7.857 mean difference

#Train
set.seed(300)
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")
grid <- expand.grid(.model = "tree",
                    .trials = c(1, 5, 10, 15, 20, 25, 30, 35),
                    .winnow = "FALSE")
model4 <- train(price ~ ., data = train_diamonds, method = "lm", trControl = ctrl)
model4
summary(model4)
prediction_values_lm4 <- predict(model4, test_diamonds)
summary(prediction_values_lm4)
actual_vs_expected_diff_lm4 <- test_diamonds$price - prediction_values_lm4
summary(actual_vs_expected_diff_lm4)
# RMSE      Rsquared   MAE     
#  1117.572  0.9211463  732.9271
# 5.862 mean difference 
```

In order to create the best fitting linear regression model to predict price based on diamond characteristics, we first created the main effects model including all predictors, which had an RMSE of 1162.9890409, an R squared of 0.9158249 and MAE of 744.0980344. Next, we attempted to improve the model by assessing whether quadratic terms, log transformations, or interactions would help. When plotting the quantitative predictors against price, it appeared that the variable x had a quadratic relationship with price, so we added a quadratic term for this variable to the model. We also created histograms for the quantitative variables and found that the variable carat had a heavy right skew and may benefit from a log transformation. Additionally, we created some interaction plots, so by looking at these plots and just trying out different interactions in the model that intuitively made sense to us, we arrived at model 3. Model 3 has an RMSE of 1033.2668851, which is lower than the main effects model and other models we tried, an R squared of 0.9335579 which is higher than the main effects model and other models we tried, and an MAE of 744.0980344 which is lower than the main effects model and other models we tried. This model seems to best maximize all of these key metrics and do the best job at predicting price. 