---
title: "Diamond Data Analysis"
author: "Ally Weingarden, Juliet Niebylski, Ziying Peng, Leah Sun, Marisa Wong"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load Libraries

```{r}
library(ggplot2)
library(caret)
library(neuralnet)
library(rpart)
library(kernlab)
library(randomForest)
```

### Load in Data

```{r, cache = TRUE}
diamonds <- diamonds
summary(diamonds)
str(diamonds)
nrow(diamonds)
```

Data is from the ggplot2 library in R.

For the second group project, we have chosen a dataset on diamonds. This dataset has 10 variables which includes the price as well as various characteristics about diamonds. There are 53940 observations of data in this dataset. Our business question is: how should we price diamonds based on certain characteristics? We will answer this question by creating models to predict the price using a variety of characteristics about diamonds. This addresses the business problem of setting prices for a seller's inventory of diamonds since it is important to set reasonable prices based on what other similar diamonds are selling for.

### Clean Data

```{r, cache = TRUE}
# Make certain variables to be a factor 
# cut, color, clarity 
diamonds$cut <- factor(diamonds$cut, ordered = FALSE)
diamonds$color <- factor(diamonds$color, ordered = FALSE)
diamonds$clarity <- factor(diamonds$clarity, ordered = FALSE)

str(diamonds)
summary(diamonds)
```

Before moving to the analysis of the data set, we cleaned the data to prepare it for regression. Cut, color, and clarity were ordinal factors. For creating our models, we decided to change cut, color, and clarity to be factors which are not ordered.

### Regression

```{r, cache + TRUE}
diamonds_model <- lm(price ~ ., data = diamonds)
summary(diamonds_model)
```

We ran a linear regression model on the cleaned data with the response variable price and included all predictor variables. From doing this, we see that only variables y and z are not statistically significant at an alpha level of 0.05. All other predictor variables are statistically significant at an alpha level of 0.05 (and all of those are even statistically significant at an alpha level of 0.001). Because of having many statistically significant predictor variables at an alpha level of 0.05, we decided to go ahead and choose this dataset to use in this project to predict the price of diamonds.

### Split Data into Training and Testing Data

```{r, cache = TRUE}
# Get value of 70% of the data
num_training_data <- round(0.7 * nrow(diamonds))

# Get random sample of indexes for the training data
set.seed(123) # Set a seed
train_index <- sample(x = c(1:nrow(diamonds)), size = num_training_data, replace = FALSE)

# Get training data based on the train_index values randomly selected
train_diamonds <- diamonds[train_index,]
# Get testing data
test_diamonds <- diamonds[-train_index,]
# Check: make sure the number of values in training set and testing set 
# add to the number of rows in the original dataset
nrow(diamonds) == nrow(train_diamonds) + nrow(test_diamonds)
# Get summary of training and test data, make sure similar distribution of price values
summary(train_diamonds)
summary(test_diamonds)
```

### Do Linear Regression on Training Set, Predict Price for Testing Set

```{r, cache = TRUE}
# Main effects model
model1 <- lm(price ~ ., train_diamonds)
summary(model1)
# Predict price for testing data
prediction_values_lm1 <- predict(model1, test_diamonds)
summary(prediction_values_lm1)
# Calculate difference between actual price and predicted price
actual_vs_expected_diff_lm1 <- test_diamonds$price - prediction_values_lm1
summary(actual_vs_expected_diff_lm1)

postResample(prediction_values_lm1, test_diamonds$price)
```

### KNN Regression

```{r, cache = TRUE}
knnreg_model1 <- knnreg(price ~ ., data = train_diamonds, k = 5)
summary(knnreg_model1)
prediction_values_knn1 <- predict(knnreg_model1, test_diamonds)
summary(prediction_values_knn1)
actual_vs_expected_diff_knn1 <- test_diamonds$price - prediction_values_knn1
summary(actual_vs_expected_diff_knn1)

postResample(prediction_values_knn1, test_diamonds$price)
#        RMSE     Rsquared          MAE 
#1037.3289972    0.9343628  540.2361728 

# # Improving model through changing k
# knnreg_model2 <- knnreg(price ~ ., data = train_diamonds, k = 10)
# prediction_values_knn2 <- predict(knnreg_model2, test_diamonds)
# actual_vs_expected_diff_knn2 <- test_diamonds$price - prediction_values_knn2
# 
# postResample(prediction_values_knn2, test_diamonds$price)
# #        RMSE     Rsquared          MAE 
# #1056.7784212    0.9343528  553.3987039 
# 
# knnreg_model3 <- knnreg(price ~ ., data = train_diamonds, k = 7)
# prediction_values_knn3 <- predict(knnreg_model3, test_diamonds)
# actual_vs_expected_diff_knn3 <- test_diamonds$price - prediction_values_knn3
# 
# postResample(prediction_values_knn3, test_diamonds$price)
# #        RMSE     Rsquared          MAE 
# #1039.6573549    0.9349978  542.4383039 
# 
# knnreg_model4 <- knnreg(price ~ ., data = train_diamonds, k = 20)
# prediction_values_knn4 <- predict(knnreg_model4, test_diamonds)
# actual_vs_expected_diff_knn4 <- test_diamonds$price - prediction_values_knn4
# 
# postResample(prediction_values_knn4, test_diamonds$price)
# #        RMSE     Rsquared          MAE 
# #1100.2318281    0.9311135  580.5150942 
```

# ANN 

```{r, cache = TRUE}
# need to break into dummy variables
ANN_model1 <- neuralnet(formula = price ~ ., data = train_diamonds)
```

### SVM

```{r, cache = TRUE}
SVM_model1 <- ksvm(price ~ ., data = train_diamonds, kernel = "vanilladot")
prediction_values_SVM1 <- predict(SVM_model1, test_diamonds)
postResample(prediction_values_SVM1, test_diamonds$price)
```

### Regression Tree

Info about regression trees: https://www.datadriveninvestor.com/2020/04/13/how-do-regression-trees-work/

```{r, cache = TRUE}
# One regression tree made with training data to predict price of testing data
reg_tree_1 <- rpart(formula = price ~ ., data = train_diamonds)
prediction_values_reg_tree1 <- predict(reg_tree_1, test_diamonds)
postResample(prediction_values_reg_tree1, test_diamonds$price)
```

### Random Forest

#### One random forest made with training data to predict price of testing data

```{r, cache = TRUE}
random_forest_model1 <- randomForest(price ~ ., data = train_diamonds, ntree = 10)
prediction_values_random_forest1 <- predict(random_forest_model1, test_diamonds)
postResample(prediction_values_random_forest1, test_diamonds$price)
```

#### Tuning random forest

method infor for train function: https://topepo.github.io/caret/train-models-by-tag.html 
Random forest info for train function: https://topepo.github.io/caret/train-models-by-tag.html#random-forest 

```{r, cache = TRUE}
#ntree_rf <- data.frame("ntree" = seq(10, 1000, 10))
tune_grid_rf <- data.frame("mtry" = c(2:3))
ctrl_rf <- trainControl(method = "cv", 10) # use 10-fold cross-validation
set.seed(12345)
#random_forests <- train(price ~ ., data = diamonds, method = "rf", tuneGrid = tune_grid_rf, trControl = ctrl_rf, ntree = ntree_rf)
random_forests <- train(price ~ ., data = diamonds, method = "rf", tuneGrid = tune_grid_rf, trControl = ctrl_rf, ntree = 10)
random_forests
```


QUESTIONS:
1. Why are there negative prices predicted? 

ways to censor the data, don't worry about this right now (for questions 1 and 2)

2. Is there a way to ensure that the price is positive?

3. How to test goodness of estimation?

caret package: use RMSE (lower is better), MAE (lower is better), R^2 (larger is better) postResample in Caret (will calculate all three metrics)

Resources:
RMSE: https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/
MAE: https://www.statisticshowto.com/absolute-error/ 
R^2: https://www.investopedia.com/terms/r/r-squared.asp 

4. Not sure how accurate the data is since couldn't find info about using the letter color scale and clarity coding that the dataset uses for cubic zirconia (seems like they are using diamond coding). Should we instead use the diamond dataset the professor had mentioned?

switch over to diamonds data from ggplot2, don't need to redo the first deliverable

5. When are the other deliverables due?

- second deliverable due sometime before Thanksgiving

Model specifications:

- Linear models (can include interactions and non-linear terms) <- Ally
- KNN reg package, use KNNreg function in caret <- Ziying
- neural network (use as normal) <- Leah
- decision tree (regression trees), package CART or PARTYCIT RPART  <- Juliet
- SVM (support vector machines) (try to use as normal) <- Marisa
- random forest for continuous data, RPART or CART <- Juliet

Regression tree resource: https://www.geeksforgeeks.org/decision-tree-for-regression-in-r-programming/

Aside: Resources on Cubic Zirconia:
https://www.firemountaingems.com/resources/encyclobeadia/gem-notes/hb1f

https://www.thespruce.com/cubic-zirconia-information-2043824 

https://www.jewelry-secrets.com/Blog/cubic-zirconia-grading/ 

Info about diamonds: https://www.forevermark.com/en-us/now-forever/guides/diamond-engagement-guide/diamond-color-and-clarity/
